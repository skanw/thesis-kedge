#!/usr/bin/env python3
"""Real web crawler with proper compliance and robots.txt respect."""

import asyncio
import aiohttp
import time
import random
from pathlib import Path
from urllib.robotparser import RobotFileParser
from urllib.parse import urljoin, urlparse
import structlog
import pandas as pd
from typing import List, Dict, Optional, Set
import json
from datetime import datetime
import re

logger = structlog.get_logger()

class RealWebCrawler:
    """Real web crawler with compliance and ethical practices."""
    
    def __init__(self, config: Dict):
        """Initialize crawler with configuration."""
        self.config = config
        self.session = None
        self.robots_cache = {}
        self.rate_limiter = AdaptiveRPS(
            rps=config.get('rate_limit_rps', 0.5),
            min_rps=config.get('min_rps', 0.1),
            max_rps=config.get('max_rps', 1.0)
        )
        self.user_agent = config.get('user_agent', 
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')
        
    async def __aenter__(self):
        """Async context manager entry."""
        connector = aiohttp.TCPConnector(limit=10, limit_per_host=2)
        timeout = aiohttp.ClientTimeout(total=30, connect=10)
        self.session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout,
            headers={'User-Agent': self.user_agent}
        )
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit."""
        if self.session:
            await self.session.close()
    
    def check_robots_txt(self, base_url: str, path: str) -> bool:
        """Check if path is allowed by robots.txt."""
        try:
            domain = urlparse(base_url).netloc
            robots_url = f"https://{domain}/robots.txt"
            
            if domain not in self.robots_cache:
                rp = RobotFileParser()
                rp.set_url(robots_url)
                rp.read()
                self.robots_cache[domain] = rp
                logger.info(f"Loaded robots.txt for {domain}")
            
            rp = self.robots_cache[domain]
            can_fetch = rp.can_fetch(self.user_agent, path)
            
            if not can_fetch:
                logger.warning(f"Robots.txt disallows: {path} on {domain}")
            
            return can_fetch
            
        except Exception as e:
            logger.error(f"Error checking robots.txt for {base_url}: {e}")
            # If we can't check robots.txt, be conservative
            return False
    
    async def fetch_page(self, url: str) -> Optional[Dict]:
        """Fetch a single page with rate limiting and error handling."""
        try:
            # Check robots.txt first
            parsed_url = urlparse(url)
            if not self.check_robots_txt(url, parsed_url.path):
                return None
            
            # Rate limiting
            self.rate_limiter.wait()
            
            async with self.session.get(url) as response:
                # Adaptive rate limiting based on response
                self.rate_limiter.feedback(response.status)
                
                if response.status == 200:
                    content = await response.text()
                    return {
                        'url': url,
                        'content': content,
                        'status': response.status,
                        'headers': dict(response.headers),
                        'timestamp': datetime.now().isoformat()
                    }
                elif response.status in [403, 429, 503]:
                    logger.warning(f"Rate limited or blocked: {url} (status: {response.status})")
                    return None
                else:
                    logger.warning(f"HTTP error: {url} (status: {response.status})")
                    return None
                    
        except asyncio.TimeoutError:
            logger.error(f"Timeout fetching: {url}")
            return None
        except Exception as e:
            logger.error(f"Error fetching {url}: {e}")
            return None
    
    def extract_product_links(self, content: str, base_url: str) -> List[str]:
        """Extract product links from page content."""
        # This is a simplified extractor - in practice you'd use BeautifulSoup
        # and site-specific selectors
        import re
        
        # Common product URL patterns
        patterns = [
            r'href="([^"]*product[^"]*)"',
            r'href="([^"]*p/[^"]*)"',
            r'href="([^"]*produit[^"]*)"',  # French
        ]
        
        links = set()
        for pattern in patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            for match in matches:
                full_url = urljoin(base_url, match)
                if self.is_product_url(full_url):
                    links.add(full_url)
        
        return list(links)
    
    def is_product_url(self, url: str) -> bool:
        """Check if URL looks like a product page."""
        product_indicators = [
            '/product/', '/p/', '/produit/', '/item/',
            'product-', 'produit-', 'item-'
        ]
        return any(indicator in url.lower() for indicator in product_indicators)
    
    def extract_product_data(self, content: str, url: str) -> Optional[Dict]:
        """Extract product data from page content."""
        # This is a simplified extractor - in practice you'd use BeautifulSoup
        # and site-specific selectors for each retailer
        
        try:
            # Basic extraction patterns (would need to be customized per site)
            data = {
                'url': url,
                'scrape_ts': datetime.now().isoformat(),
                'source_url': url,
                'robots_snapshot_id': f"robots_{int(time.time())}"
            }
            
            # Extract title (simplified)
            title_match = re.search(r'<title[^>]*>([^<]+)</title>', content, re.IGNORECASE)
            if title_match:
                data['name'] = title_match.group(1).strip()
            
            # Extract price (simplified)
            price_match = re.search(r'(\d+[,.]?\d*)\s*â‚¬', content)
            if price_match:
                data['price_value'] = float(price_match.group(1).replace(',', '.'))
                data['price_currency'] = 'EUR'
            
            # Extract brand (simplified)
            brand_match = re.search(r'brand[^>]*>([^<]+)</', content, re.IGNORECASE)
            if brand_match:
                data['brand'] = brand_match.group(1).strip()
            
            # Check for refillable indicators
            refill_indicators = ['recharge', 'refill', 'rechargeable', 'rechargeable']
            refill_evidence = []
            for indicator in refill_indicators:
                if indicator.lower() in content.lower():
                    refill_evidence.append(indicator)
            
            data['refillable_flag'] = len(refill_evidence) > 0
            data['refill_evidence'] = refill_evidence
            
            return data
            
        except Exception as e:
            logger.error(f"Error extracting product data from {url}: {e}")
            return None
    
    def extract_reviews(self, content: str, product_url: str) -> List[Dict]:
        """Extract reviews from page content."""
        # This is a simplified extractor - in practice you'd look for
        # review containers, ratings, dates, etc.
        
        reviews = []
        try:
            # Look for review patterns (simplified)
            review_patterns = [
                r'rating[^>]*>(\d+)',
                r'star[^>]*>(\d+)',
                r'note[^>]*>(\d+)'
            ]
            
            for pattern in review_patterns:
                matches = re.findall(pattern, content, re.IGNORECASE)
                for match in matches:
                    try:
                        rating = int(match)
                        if 1 <= rating <= 5:
                            reviews.append({
                                'product_url': product_url,
                                'rating': rating,
                                'text': f"Review text extracted from {product_url}",
                                'language': 'fr',  # Assuming French sites
                                'review_date': datetime.now().isoformat(),
                                'source_url': product_url,
                                'scrape_ts': datetime.now().isoformat()
                            })
                    except ValueError:
                        continue
            
            return reviews[:10]  # Limit to 10 reviews per page for now
            
        except Exception as e:
            logger.error(f"Error extracting reviews from {product_url}: {e}")
            return []
    
    async def crawl_site(self, base_url: str, max_pages: int = 10) -> Dict:
        """Crawl a site for products and reviews."""
        logger.info(f"Starting crawl of {base_url} (max {max_pages} pages)")
        
        # Start with category or brand pages
        start_urls = [
            f"{base_url}/parfums",
            f"{base_url}/maquillage", 
            f"{base_url}/soin",
            f"{base_url}/marques"
        ]
        
        all_products = []
        all_reviews = []
        visited_urls = set()
        
        for start_url in start_urls:
            if len(visited_urls) >= max_pages:
                break
                
            logger.info(f"Fetching start URL: {start_url}")
            page_data = await self.fetch_page(start_url)
            
            if not page_data:
                continue
                
            visited_urls.add(start_url)
            
            # Extract product links
            product_links = self.extract_product_links(page_data['content'], base_url)
            logger.info(f"Found {len(product_links)} product links on {start_url}")
            
            # Fetch product pages
            for product_url in product_links[:5]:  # Limit to 5 products per category
                if product_url in visited_urls:
                    continue
                    
                logger.info(f"Fetching product: {product_url}")
                product_data = await self.fetch_page(product_url)
                
                if not product_data:
                    continue
                    
                visited_urls.add(product_url)
                
                # Extract product data
                product_info = self.extract_product_data(product_data['content'], product_url)
                if product_info:
                    all_products.append(product_info)
                
                # Extract reviews
                reviews = self.extract_reviews(product_data['content'], product_url)
                all_reviews.extend(reviews)
                
                # Rate limiting between products
                await asyncio.sleep(random.uniform(1, 3))
        
        return {
            'products': all_products,
            'reviews': all_reviews,
            'total_pages_visited': len(visited_urls),
            'base_url': base_url
        }

# Import the AdaptiveRPS class
from src.utils.adaptive_rps import AdaptiveRPS

async def main():
    """Main function for testing the crawler."""
    config = {
        'rate_limit_rps': 0.5,
        'min_rps': 0.1,
        'max_rps': 1.0,
        'user_agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
    }
    
    # Test with a compliant site
    test_sites = [
        'https://www.marionnaud.fr',
        'https://www.nocibe.fr'
    ]
    
    async with RealWebCrawler(config) as crawler:
        for site in test_sites:
            try:
                logger.info(f"Testing crawl of {site}")
                results = await crawler.crawl_site(site, max_pages=3)
                
                logger.info(f"Results for {site}:")
                logger.info(f"  Products: {len(results['products'])}")
                logger.info(f"  Reviews: {len(results['reviews'])}")
                logger.info(f"  Pages visited: {results['total_pages_visited']}")
                
                # Save results
                if results['products']:
                    df_products = pd.DataFrame(results['products'])
                    df_products.to_parquet(f'data/silver/products_{urlparse(site).netloc}.parquet', index=False)
                
                if results['reviews']:
                    df_reviews = pd.DataFrame(results['reviews'])
                    df_reviews.to_parquet(f'data/silver/reviews_{urlparse(site).netloc}.parquet', index=False)
                
            except Exception as e:
                logger.error(f"Error crawling {site}: {e}")

if __name__ == "__main__":
    asyncio.run(main())
